{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13207608,"sourceType":"datasetVersion","datasetId":8370965}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# установка библиотек","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\nimport subprocess\nimport sys\n\ndef install_dependencies():\n    \n    dependencies = [\n        \"PyPDF2\",\n        \"python-docx\", \n        \"pdfminer.six\",\n        \"ebooklib\",\n        \"beautifulsoup4\",\n        \"html2text\",\n        \"pandas\",\n        \"openpyxl\",\n        \"lxml\",\n        \"chardet\",\n        \"python-magic\"\n    ]\n    \n    print(\"Установка зависимостей для универсального парсера файлов...\")\n    print(\"=\" * 60)\n    \n    for package in dependencies:\n        try:\n            print(f\"Устанавливаю {package}...\")\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n            print(f\"{package} успешно установлен\")\n        except subprocess.CalledProcessError as e:\n            print(f\"✗ Ошибка при установке {package}: {e}\")\n        print(\"-\" * 40)\n    \n    print(\"\\nВсе зависимости установлены!\")\n    print(\"Можно запускать парсер: python universal_parser.py\")\n\nif __name__ == \"__main__\":\n    install_dependencies()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:22:00.354957Z","iopub.execute_input":"2025-09-29T13:22:00.355615Z","iopub.status.idle":"2025-09-29T13:22:37.347074Z","shell.execute_reply.started":"2025-09-29T13:22:00.355589Z","shell.execute_reply":"2025-09-29T13:22:37.346414Z"}},"outputs":[{"name":"stdout","text":"Установка зависимостей для универсального парсера файлов...\n============================================================\nУстанавливаю PyPDF2...\nCollecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.6/232.6 kB 4.7 MB/s eta 0:00:00\nInstalling collected packages: PyPDF2\nSuccessfully installed PyPDF2-3.0.1\nPyPDF2 успешно установлен\n----------------------------------------\nУстанавливаю python-docx...\nCollecting python-docx\n  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\nRequirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\nDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 253.0/253.0 kB 5.6 MB/s eta 0:00:00\nInstalling collected packages: python-docx\nSuccessfully installed python-docx-1.2.0\npython-docx успешно установлен\n----------------------------------------\nУстанавливаю pdfminer.six...\nCollecting pdfminer.six\n  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.2)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (44.0.3)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\nDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 47.7 MB/s eta 0:00:00\nInstalling collected packages: pdfminer.six\nSuccessfully installed pdfminer.six-20250506\npdfminer.six успешно установлен\n----------------------------------------\nУстанавливаю ebooklib...\nCollecting ebooklib\n  Downloading ebooklib-0.19-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from ebooklib) (5.4.0)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from ebooklib) (1.17.0)\nDownloading ebooklib-0.19-py3-none-any.whl (39 kB)\nInstalling collected packages: ebooklib\nSuccessfully installed ebooklib-0.19\nebooklib успешно установлен\n----------------------------------------\nУстанавливаю beautifulsoup4...\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\nbeautifulsoup4 успешно установлен\n----------------------------------------\nУстанавливаю html2text...\nCollecting html2text\n  Downloading html2text-2025.4.15-py3-none-any.whl.metadata (4.1 kB)\nDownloading html2text-2025.4.15-py3-none-any.whl (34 kB)\nInstalling collected packages: html2text\nSuccessfully installed html2text-2025.4.15\nhtml2text успешно установлен\n----------------------------------------\nУстанавливаю pandas...\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\npandas успешно установлен\n----------------------------------------\nУстанавливаю openpyxl...\nRequirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\nRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\nopenpyxl успешно установлен\n----------------------------------------\nУстанавливаю lxml...\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\nlxml успешно установлен\n----------------------------------------\nУстанавливаю chardet...\nRequirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (5.2.0)\nchardet успешно установлен\n----------------------------------------\nУстанавливаю python-magic...\nCollecting python-magic\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\nDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nInstalling collected packages: python-magic\nSuccessfully installed python-magic-0.4.27\npython-magic успешно установлен\n----------------------------------------\n\nВсе зависимости установлены!\nМожно запускать парсер: python universal_parser.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Парсер доков","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport magic\nimport chardet\nfrom typing import Dict, Any, List\nimport PyPDF2\nfrom docx import Document\nimport pdfminer.high_level\nfrom ebooklib import epub\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\nclean_text = \"\"\n\nclass TextProcessor:\n    @staticmethod\n    def clean_text(text: str) -> str:\n        text = re.sub(r'\\s+', ' ', text)\n        return text.strip()\n    \n    @staticmethod\n    def split_into_sentences(text: str) -> List[str]:\n        sentences = re.split(r'[.!?]+', text)\n        return [s.strip() for s in sentences if s.strip()]\n    \n    @staticmethod\n    def get_word_frequency(text: str, top_n: int = 10) -> Dict[str, int]:\n        words = text.lower().split()\n        word_count = {}\n        for word in words:\n            word = re.sub(r'[^\\w]', '', word)\n            if word and len(word) > 2:\n                word_count[word] = word_count.get(word, 0) + 1\n        return dict(sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:top_n])\n    \n    @staticmethod\n    def detect_encoding(file_path: str) -> str:\n        with open(file_path, 'rb') as file:\n            raw_data = file.read(10000)\n            result = chardet.detect(raw_data)\n            return result['encoding'] or 'utf-8'\n\nclass UniversalParser:\n    def __init__(self):\n        self.supported_formats = {\n            '.txt': self.parse_txt,\n            '.pdf': self.parse_pdf,\n            '.docx': self.parse_docx,\n            '.epub': self.parse_epub,\n            '.html': self.parse_html,\n            '.htm': self.parse_html,\n            '.csv': self.parse_csv,\n            '.xlsx': self.parse_excel,\n            '.xls': self.parse_excel,\n        }\n        self.text_processor = TextProcessor()\n    \n    def get_file_extension(self, file_path: str) -> str:\n        return os.path.splitext(file_path)[1].lower()\n    \n    def detect_file_type(self, file_path: str) -> str:\n        try:\n            return magic.from_file(file_path, mime=True)\n        except:\n            return \"unknown\"\n    \n    def is_supported(self, file_path: str) -> bool:\n        ext = self.get_file_extension(file_path)\n        return ext in self.supported_formats\n    \n    def get_supported_formats(self) -> List[str]:\n        return list(self.supported_formats.keys())\n    \n    def parse_txt(self, file_path: str) -> Dict[str, Any]:\n        try:\n            encoding = self.text_processor.detect_encoding(file_path)\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as file:\n                content = file.read()\n            cleaned_content = self.text_processor.clean_text(content)\n            return {\n                'content': cleaned_content,\n                'metadata': {'encoding': encoding, 'lines': len(content.split('\\n')), 'words': len(content.split()), 'characters': len(content)}\n            }\n        except Exception as e:\n            raise Exception(f\"TXT error: {str(e)}\")\n    \n    def parse_pdf(self, file_path: str) -> Dict[str, Any]:\n        try:\n            text_content = \"\"\n            metadata = {}\n            with open(file_path, 'rb') as file:\n                pdf_reader = PyPDF2.PdfReader(file)\n                if hasattr(pdf_reader, 'metadata') and pdf_reader.metadata:\n                    metadata = {\n                        'title': str(pdf_reader.metadata.get('/Title', '')),\n                        'author': str(pdf_reader.metadata.get('/Author', '')),\n                        'pages': len(pdf_reader.pages)\n                    }\n                for page in pdf_reader.pages:\n                    if hasattr(page, 'extract_text'):\n                        text_content += page.extract_text() + \"\\n\"\n            if not text_content.strip():\n                text_content = pdfminer.high_level.extract_text(file_path)\n            cleaned_content = self.text_processor.clean_text(text_content)\n            return {'content': cleaned_content, 'metadata': metadata}\n        except Exception as e:\n            raise Exception(f\"PDF error: {str(e)}\")\n    \n    def parse_docx(self, file_path: str) -> Dict[str, Any]:\n        try:\n            doc = Document(file_path)\n            content = []\n            for paragraph in doc.paragraphs:\n                if paragraph.text.strip():\n                    content.append(paragraph.text)\n            for table in doc.tables:\n                for row in table.rows:\n                    for cell in row.cells:\n                        if cell.text.strip():\n                            content.append(cell.text)\n            full_text = \"\\n\".join(content)\n            cleaned_content = self.text_processor.clean_text(full_text)\n            core_properties = doc.core_properties\n            return {\n                'content': cleaned_content,\n                'metadata': {\n                    'title': core_properties.title,\n                    'author': core_properties.author,\n                    'paragraphs': len(doc.paragraphs),\n                    'tables': len(doc.tables)\n                }\n            }\n        except Exception as e:\n            raise Exception(f\"DOCX error: {str(e)}\")\n    \n    def parse_epub(self, file_path: str) -> Dict[str, Any]:\n        try:\n            book = epub.read_epub(file_path)\n            content = []\n            for item in book.get_items():\n                if item.get_type() == epub.ITEM_DOCUMENT:\n                    soup = BeautifulSoup(item.get_content(), 'html.parser')\n                    text = soup.get_text(separator='\\n')\n                    if text.strip():\n                        content.append(text)\n            full_text = \"\\n\\n\".join(content)\n            cleaned_content = self.text_processor.clean_text(full_text)\n            metadata = {}\n            if book.get_metadata('DC', 'title'):\n                metadata['title'] = book.get_metadata('DC', 'title')[0][0]\n            if book.get_metadata('DC', 'creator'):\n                metadata['author'] = book.get_metadata('DC', 'creator')[0][0]\n            return {'content': cleaned_content, 'metadata': metadata}\n        except Exception as e:\n            raise Exception(f\"EPUB error: {str(e)}\")\n    \n    def parse_html(self, file_path: str) -> Dict[str, Any]:\n        try:\n            encoding = self.text_processor.detect_encoding(file_path)\n            with open(file_path, 'r', encoding=encoding, errors='ignore') as file:\n                html_content = file.read()\n            soup = BeautifulSoup(html_content, 'html.parser')\n            for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n                script.decompose()\n            text = soup.get_text(separator='\\n')\n            lines = (line.strip() for line in text.splitlines())\n            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n            cleaned_text = '\\n'.join(chunk for chunk in chunks if chunk)\n            links = [a.get('href') for a in soup.find_all('a', href=True)]\n            return {\n                'content': cleaned_text,\n                'metadata': {\n                    'title': soup.title.string if soup.title else '',\n                    'links_count': len(links),\n                    'encoding': encoding\n                }\n            }\n        except Exception as e:\n            raise Exception(f\"HTML error: {str(e)}\")\n    \n    def parse_csv(self, file_path: str) -> Dict[str, Any]:\n        try:\n            with open(file_path, 'r', encoding='utf-8') as file:\n                first_line = file.readline()\n            if ';' in first_line:\n                delimiter = ';'\n            elif ',' in first_line:\n                delimiter = ','\n            elif '\\t' in first_line:\n                delimiter = '\\t'\n            else:\n                delimiter = ','\n            df = pd.read_csv(file_path, delimiter=delimiter, encoding='utf-8', on_bad_lines='skip')\n            text_content = df.to_string()\n            return {\n                'content': text_content,\n                'metadata': {\n                    'rows': len(df),\n                    'columns': len(df.columns),\n                    'columns_names': list(df.columns),\n                    'delimiter': delimiter\n                }\n            }\n        except Exception as e:\n            raise Exception(f\"CSV error: {str(e)}\")\n    \n    def parse_excel(self, file_path: str) -> Dict[str, Any]:\n        try:\n            excel_file = pd.ExcelFile(file_path)\n            sheets_data = {}\n            sheets_info = []\n            for sheet_name in excel_file.sheet_names:\n                df = pd.read_excel(file_path, sheet_name=sheet_name)\n                sheet_content = df.to_string()\n                sheets_data[sheet_name] = sheet_content\n                sheets_info.append({'name': sheet_name, 'rows': len(df), 'columns': len(df.columns)})\n            full_content = \"\\n\\n\".join([f\"=== {name} ===\\n{content}\" for name, content in sheets_data.items()])\n            return {\n                'content': full_content,\n                'metadata': {\n                    'sheets_count': len(excel_file.sheet_names),\n                    'sheets_info': sheets_info,\n                    'sheet_names': excel_file.sheet_names\n                }\n            }\n        except Exception as e:\n            raise Exception(f\"Excel error: {str(e)}\")\n    \n    def parse_file(self, file_path: str) -> Dict[str, Any]:\n        if not os.path.exists(file_path):\n            raise FileNotFoundError(f\"File not found: {file_path}\")\n        if not os.path.isfile(file_path):\n            raise ValueError(f\"Not a file: {file_path}\")\n        ext = self.get_file_extension(file_path)\n        if not self.is_supported(file_path):\n            detected_type = self.detect_file_type(file_path)\n            raise ValueError(f\"Unsupported format: {ext} (detected: {detected_type})\")\n        parser_func = self.supported_formats[ext]\n        return parser_func(file_path)\n    \n    def extract_text_only(self, file_path: str) -> str:\n        result = self.parse_file(file_path)\n        return result['content']\n    \n    def get_statistics(self, file_path: str) -> Dict[str, Any]:\n        result = self.parse_file(file_path)\n        content = result['content']\n        stats = {\n            'file_name': os.path.basename(file_path),\n            'file_size': os.path.getsize(file_path),\n            'file_format': self.get_file_extension(file_path),\n            'detected_type': self.detect_file_type(file_path),\n            'lines': len(content.split('\\n')),\n            'words': len(content.split()),\n            'characters': len(content),\n            'unique_words': len(set(content.split())),\n            'sentences': len(self.text_processor.split_into_sentences(content))\n        }\n        word_freq = self.text_processor.get_word_frequency(content, 5)\n        stats['top_words'] = word_freq\n        if 'metadata' in result:\n            stats.update({'file_' + k: v for k, v in result['metadata'].items()})\n        return stats\n    \n    def search_in_file(self, file_path: str, search_term: str, case_sensitive: bool = False) -> List[Dict]:\n        content = self.extract_text_only(file_path)\n        if not case_sensitive:\n            content_lower = content.lower()\n            search_term_lower = search_term.lower()\n        else:\n            content_lower = content\n            search_term_lower = search_term\n        lines = content.split('\\n')\n        results = []\n        for i, line in enumerate(lines, 1):\n            if case_sensitive:\n                search_content = line\n            else:\n                search_content = line.lower()\n            if search_term_lower in search_content:\n                start = 0\n                while True:\n                    pos = search_content.find(search_term_lower, start)\n                    if pos == -1:\n                        break\n                    results.append({\n                        'line_number': i,\n                        'position': pos + 1,\n                        'context': line[max(0, pos-20):pos+len(search_term)+20],\n                        'match': line[pos:pos+len(search_term)]\n                    })\n                    start = pos + 1\n        return results\n\ndef main():\n    global clean_text\n    parser = UniversalParser()\n    print(\"Supported formats:\", \", \".join(parser.get_supported_formats()))\n    \n    file_path = input(\"Enter file path: \").strip().strip('\"')\n    \n    try:\n        if parser.is_supported(file_path):\n            result = parser.parse_file(file_path)\n            clean_text = result['content']\n            print(clean_text)\n\n        else:\n            print(\"Unsupported file format\")\n            \n    except Exception as e:\n        print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:22:37.348196Z","iopub.execute_input":"2025-09-29T13:22:37.348433Z","iopub.status.idle":"2025-09-29T13:22:47.198513Z","shell.execute_reply.started":"2025-09-29T13:22:37.348415Z","shell.execute_reply":"2025-09-29T13:22:47.197871Z"}},"outputs":[{"name":"stdout","text":"Supported formats: .txt, .pdf, .docx, .epub, .html, .htm, .csv, .xlsx, .xls\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter file path:  /kaggle/input/oleg-dorkanov-resume/Oleg Drokanov CV EN.pdf\n"},{"name":"stdout","text":"Oleg Drokanov DoB 12.03.2009 , Saint Petersburg , Russia +79214054678 | oleg.drokanov@yandex.ru | https: //github.com/olegg366 Key Information I am developer and researcher in the field of artificial intelligence, machine learning, and robotics. I enjoy solving complex problems and turning ideas into working projects —from experimenting with neural networks to buil ding real robotic systems. I have successful experience in international olympiads and practical experience in implementing full -scale AI solutions. Achievements and Awards • Gold Medal | IOAI 2025 (International Olympiad in Artificial Intelligence) • 1st Place | International AI Challenge, \"Education\" case (2024) • Prize Winner | DANO, Data Analysis Olympiad (2024) • Prize Winner | All-Russian School Olympiad in Technology, \"Robotics\" track (2025) • 4th Place | NEOAI (Northern Eurasia Olympiad in Artificial Intelligence, 2025) • 2nd Place | Russian Robotics Olympiad, \"Creative Category . AI\", project: \"AI Artist Robot\" (2024) • 1st Place | International Festival \"RoboFin ist\", \"Creative Category\", project: \"AI Artist Robot\" (2024) Education and Courses • Presidential Physics and Mathematics Lyceum No. 239 | Saint Petersburg 11th-grade student, speci alization: Computer Science (expected graduation: 2026) • Preparation for the International Olympiad in AI (IOAI) | Central University and MIPT (2025) • Data Analysis Specialization | Yandex School of Data Analysis (2024) • Competitive Prog ramming Courses | ITMO University (2023) • Advanced Mathematics and Programming Courses | Online Center \"Sirius\" (2022 –2023) Technical Skills • Programming Languages: Python (Proficient), C++ (Basic) • ML/DL Frameworks: PyTorch, Keras, TensorRT, Scikit -learn, Hugging Face Transformers, Diffusers • Libraries and Tools: Pandas, NumPy, OpenCV, Matplotlib, Seaborn, Timm, CatBoost, LightGBM • AI Concepts: CNN, RNN, LSTM, GAN, AE, Vision Transformers (ViT), CLIP, Diffusion, Transformers (BERT, GPT), LoRA, Quantization • Hardware and IoT: NVIDIA Jetson, Raspberry Pi, Arduino; sensor integration and prototyping • Development and Tools: Git, Linux/Windows Subsystem for Linux (WSL) administration, Docker • Other: 3D modeling (AutoCAD), circuit design (TinkerCAD) Projects • AI Artist Robot Developed a full system that captures hand -drawn sketches in the air using a camera, c ompletes them using a Stable Diffusion model, and draws the final image on paper using a plotter with a marker. • Real-Time Gesture Recognition System for an Anthropomorphic Robot Created and trained a CNN model for real -time gesture recognition to enable human-robot interaction in emotional support applications. • Online Course Evaluation System Using NLP Developed an NLP algorithm to analyze text reviews and automatically assign a score (from 1 to 10) to online courses, demonstrating skills in sentiment analy sis and regression. • Compound Generation Algorithm Developed a genetic algorithm using machine learning to generate new chemical structures with target solubility properties. • Data Analysis Projects ▪ E-commerce Analysis: Researched the correlation between pro duct availability metrics and purchase probability for an online clothing retailer. ▪ Household Services Analysis: Investigated factors (climate, demographics) influencing preferences in room cooling systems.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q ollama transformers accelerate sentencepiece protobuf lsprotocol ipywidgets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:22:47.199251Z","iopub.execute_input":"2025-09-29T13:22:47.199612Z","iopub.status.idle":"2025-09-29T13:24:08.388134Z","shell.execute_reply.started":"2025-09-29T13:22:47.199592Z","shell.execute_reply":"2025-09-29T13:24:08.387427Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:24:08.389903Z","iopub.execute_input":"2025-09-29T13:24:08.390146Z","iopub.status.idle":"2025-09-29T13:24:48.866391Z","shell.execute_reply.started":"2025-09-29T13:24:08.390124Z","shell.execute_reply":"2025-09-29T13:24:48.865555Z"}},"outputs":[{"name":"stdout","text":">>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n######################################################################## 100.0%                                                    8.8%###########################                            65.1%\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T13:24:48.867352Z","iopub.execute_input":"2025-09-29T13:24:48.867580Z","iopub.status.idle":"2025-09-29T13:24:48.871780Z","shell.execute_reply.started":"2025-09-29T13:24:48.867557Z","shell.execute_reply":"2025-09-29T13:24:48.871135Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# качаем оламу\nimport subprocess\nimport time\nimport threading\n\ndef run_ollama():\n    subprocess.run([\"ollama\", \"serve\"])\n\n# Запускаем в отдельном потоке\nollama_thread = threading.Thread(target=run_ollama, daemon=True)\nollama_thread.start()\n\n# Даем время на запуск\ntime.sleep(5)\n\n!ollama pull llama3.1:8b-instruct-q4_0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:16:33.529501Z","iopub.execute_input":"2025-09-29T14:16:33.529787Z","iopub.status.idle":"2025-09-29T14:16:39.021766Z","shell.execute_reply.started":"2025-09-29T14:16:33.529766Z","shell.execute_reply":"2025-09-29T14:16:39.020798Z"}},"outputs":[{"name":"stderr","text":"Error: listen tcp 127.0.0.1:11434: bind: address already in use\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2025/09/29 - 14:16:38 | 200 |      33.729µs |       127.0.0.1 | HEAD     \"/\"\n\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l[GIN] 2025/09/29 - 14:16:38 | 200 |  351.462058ms |       127.0.0.1 | POST     \"/api/pull\"\n\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\npulling 8eeb52dfb3bb: 100% ▕██████████████████▏ 4.7 GB                         \u001b[K\npulling 948af2743fc7: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\npulling 0ba8f0e314b4: 100% ▕██████████████████▏  12 KB                         \u001b[K\npulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\npulling 1a4c3c319823: 100% ▕██████████████████▏  485 B                         \u001b[K\nverifying sha256 digest \u001b[K\nwriting manifest \u001b[K\nsuccess \u001b[K\u001b[?25h\u001b[?2026l\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"# Analyze Agent","metadata":{}},{"cell_type":"code","source":"pr_aa = '''Ты — «Analyze Agent», эксперт по анализу литературного и медийного контента. Твоя задача — тщательно анализировать предоставленный текст и выделять из него ключевую информацию по заданным критериям.\n\n**ИНСТРУКЦИИ:**\n1.  Внимательно прочитай весь текст.\n2.  На основе содержания текста заполни каждый пункт приведенной ниже структуры.\n3.  Будь точен и объективен. Если какой-то пункт невозможно определить, укажи \"Неясно\" и кратко объясни почему.\n4.  Отвечай **ТОЛЬКО** в формате JSON, строго следуя приведенной структуре.\n\n**ВЫХОДНАЯ СТРУКТУРА (JSON):**\n{\n  \"scenes\": [\"Краткое описание основной сцены 1\", \"Краткое описание основной сцены 2\", ...],\n  \"characters\": [\"Имя или роль персонажа 1\", \"Имя или роль персонажа 2\", ...],\n  \"tone\": \"Общий тон повествования (например, мрачный, радостный, саркастический, напряженный)\",\n  \"target_audience\": \"Описание целевой аудитории текста (например, фанаты научной фантастики, молодые родители, студенты-историки)\",\n  \"age_rating\": \"Рекомендуемая возрастная категория (G, PG, PG-13, R, etc. или по российской системе: 0+, 6+, 12+, 16+, 18+)\",\n  \"style\": \"Преобладающий стиль (юмор, серьезный, образовательный, сатирический, драматический - можно указать несколько через запятую)\",\n  \"key_quotes\": [\"Цитата 1\", \"Цитата 2\", ...] // Выбери 3-5 самых ярких или значимых цитат.\n}\n\n**ПРИМЕР ОТВЕТА:** \n**отвечай только json файлом, ничего лишнего**\n{\n  \"scenes\": [\"Диалог в кафе между двумя шпионами\", \"Погоня по крышам ночного города\"],\n  \"characters\": [\"Агент Х\", \"Агент Y\", \"Таинственный незнакомец\"],\n  \"tone\": \"Напряженный и полный сарказма\",\n  \"target_audience\": \"Взрослые любители шпионских триллеров\",\n  \"age_rating\": \"PG-13 (16+)\",\n  \"style\": \"серьезный, драматический\",\n  \"key_quotes\": [\"«Холодная война закончилась, но мы все еще в тени», — сказал Агент Х, помешивая кофе.\", \"«На крышах нет правил, есть только гравитация», — крикнул Агент Y, прыгая через пропасть.\"]\n}\n\nНачинай анализ сразу после получения текста.\n\nвот текст для обработки: ''' + f'{clean_text}'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:10:18.218180Z","iopub.execute_input":"2025-09-29T14:10:18.218456Z","iopub.status.idle":"2025-09-29T14:10:18.223537Z","shell.execute_reply.started":"2025-09-29T14:10:18.218435Z","shell.execute_reply":"2025-09-29T14:10:18.222781Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import requests\nimport sys\n\ndef ask_ollama(question: str) -> str:\n    api_url = \"http://localhost:11434/api/generate\"\n    payload = {\n        \"model\": \"llama3.1:8b-instruct-q4_0\",\n        \"prompt\": question,\n        \"stream\": False,\n        \"options\": {\n            \"temperature\": 0.7,\n            \"num_predict\": 2000\n        }\n    }\n    \n    try:\n        response = requests.post(api_url, json=payload, timeout=500)\n        response.raise_for_status()\n        return response.json()[\"response\"]\n    except requests.exceptions.RequestException as e:\n        return f\"Ошибка запроса: {str(e)}\"\n\ndef check_ollama():\n    \"\"\"Проверяет доступность Ollama\"\"\"\n    try:\n        response = requests.get(\"http://localhost:11434/api/tags\", timeout=10)\n        if response.status_code == 200:\n            print(\"✅ Ollama работает корректно\")\n            return True\n        else:\n            print(\"❌ Ollama не отвечает\")\n            return False\n    except:\n        print(\"❌ Не удалось подключиться к Ollama\")\n        return False\n\nif __name__ == \"__main__\":\n    \n    question = pr_aa\n    \n    if check_ollama():\n        answer_aa = ask_ollama(question)\n        print(answer_aa)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:18:40.860506Z","iopub.execute_input":"2025-09-29T14:18:40.860986Z","iopub.status.idle":"2025-09-29T14:18:48.043422Z","shell.execute_reply.started":"2025-09-29T14:18:40.860961Z","shell.execute_reply":"2025-09-29T14:18:48.042670Z"}},"outputs":[{"name":"stdout","text":"[GIN] 2025/09/29 - 14:18:40 | 200 |     941.647µs |       127.0.0.1 | GET      \"/api/tags\"\n✅ Ollama работает корректно\n[GIN] 2025/09/29 - 14:18:48 | 200 |  7.170009611s |       127.0.0.1 | POST     \"/api/generate\"\n{\n  \"scenes\": [\"Сценка описания работы в области ИИ и ML\", \"Погоня за достижениями в международных олимпиадах\"],\n  \"characters\": [\"Олег Дроканов - разработчик и исследователь в области ИИ, ML и робототехники\"],\n  \"tone\": \"Упругий и профессиональный\",\n  \"target_audience\": \"Профессионалы в области ИИ, ML и робототехники, студенты-специалисты по компьютерным наукам\",\n  \"age_rating\": \"Не применимо\",\n  \"style\": \"Образовательный, технический\",\n  \"key_quotes\": [\"«Я люблю решать сложные проблемы и преобразовывать идеи в рабочие проекты», — говорит Олег.\", \"«Моя цель - не просто обучить модели, но сделать их способными принимать решения в реальных условиях».\"]\n}\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Script Writer (LLM)","metadata":{}},{"cell_type":"code","source":"pr_sw = '''Ты — «Script Writer», профессиональный сценарист и специалист по prompt engineering для генерации изображений. Твоя задача — адаптировать литературный текст в формат сценария из серии панелей (как в комиксе или раскадровке).\n\n**ИНСТРУКЦИИ:**\n1.  Разбей исходный текст на логические сцены или ключевые моменты. Каждый момент станет отдельной \"панелью\".\n2.  Для каждой панели создай:\n    *   **Номер панели:** (Panel 1, Panel 2...)\n    *   **Визуальное описание:** Яркое, детализированное описание того, что должно быть на изображении. Включи информацию о персонажах (внешность, эмоции, поза), окружении, освещении, ключевых объектах и стиле.\n    *   **Диалог/Текст:** Реплики персонажей или повествовательный текст, который будет отображаться на панели (в \"пузыре\" или как подпись). Если диалога нет, можно оставить пустым или добавить звуко-ономатопею (например, *БАМ!*).\n    *   **Image Prompt:** Готовый, оптимизированный промпт для AI-генератора изображений (например, Stable Diffusion, Midjourney). Промпт должен быть на английском языке для лучшей совместимости, детализированным и включать ключевые слова по стилю (e.g., `cinematic shot, photorealistic, dynamic lighting, detailed background`).\n3.  Стремись к визуальной насыщенности. \"Показывай, а не рассказывай\".\n4.  Отвечай **ТОЛЬКО** в формате JSON, строго следуя приведенной структуре.\n\n**ВЫХОДНАЯ СТРУКТУРА (JSON):**\n{\n  \"script_title\": \"Название сценария на основе текста\",\n  \"panels\": [\n    {\n      \"panel_number\": 1,\n      \"visual_description\": \"Детальное описание сцены на русском.\",\n      \"dialogue_text\": \"Реплики или текст для этой панели.\",\n      \"image_prompt\": \"A detailed, cinematic image of [описание сцены на английском]. Style: [указание стиля]. Lighting: [описание освещения]. --ar 16:9 --v 6.0\"\n    },\n    // ... последующие панели\n  ]\n}\n\n**ПРИМЕР ОТВЕТА:** \n**отвечай только json файлом, ничего лишнего**\n{\n  \"script_title\": \"Встреча в кафе\",\n  \"panels\": [\n    {\n      \"panel_number\": 1,\n      \"visual_description\": \"Двое мужчин в дорогих костюмах сидят за маленьким столиком в затемненном кафе. Один (Агент Х) помешивает ложечкой эспрессо, его лицо освещено лишь светом от неоновой вывески за окном. Он смотрит на собеседника с легкой усмешкой. Второй (Агент Y) напряжен, его взгляд направлен в сторону.\",\n      \"dialogue_text\": \"Агент Х: «Холодная война закончилась, но мы все еще в тени».\",\n      \"image_prompt\": \"A cinematic shot of two well-dressed spies in a moody, dark cafe at night. One man stirs his espresso with a sly smirk, neon light from a window sign illuminates his face. The other man looks tense, glancing sideways. Photorealistic, film noir style, dramatic lighting, shallow depth of field. --ar 16:9 --v 6.0\"\n    }\n    // ... Panel 2 и т.д.\n  ]\n}\n\nНачинай создание сценария сразу после получения текста.\n\nтекст для обработки:''' + f'{clean_text}'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:18:59.851788Z","iopub.execute_input":"2025-09-29T14:18:59.852437Z","iopub.status.idle":"2025-09-29T14:18:59.857263Z","shell.execute_reply.started":"2025-09-29T14:18:59.852414Z","shell.execute_reply":"2025-09-29T14:18:59.856576Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def check_ollama():\n    \"\"\"Проверяет доступность Ollama\"\"\"\n    try:\n        response = requests.get(\"http://localhost:11434/api/tags\", timeout=10)\n        if response.status_code == 200:\n            print(\"✅ Ollama работает корректно\")\n            return True\n        else:\n            print(\"❌ Ollama не отвечает\")\n            return False\n    except:\n        print(\"❌ Не удалось подключиться к Ollama\")\n        return False\n\nif __name__ == \"__main__\":\n    \n    question = pr_sw\n    \n    if check_ollama():\n        answer_sw = ask_ollama(question)\n        print(answer_sw)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-29T14:19:00.138793Z","iopub.execute_input":"2025-09-29T14:19:00.138984Z","iopub.status.idle":"2025-09-29T14:19:22.647876Z","shell.execute_reply.started":"2025-09-29T14:19:00.138969Z","shell.execute_reply":"2025-09-29T14:19:22.647273Z"}},"outputs":[{"name":"stdout","text":"[GIN] 2025/09/29 - 14:19:00 | 200 |     937.081µs |       127.0.0.1 | GET      \"/api/tags\"\n✅ Ollama работает корректно\n[GIN] 2025/09/29 - 14:19:22 | 200 | 22.497981099s |       127.0.0.1 | POST     \"/api/generate\"\n**Сценарий с использованием панелей**\n\n\n{\n  \"script_title\": \"Портфолио разработчика\",\n  \"panels\": [\n    {\n      \"panel_number\": 1,\n      \"visual_description\": \"Молодой парень (Oleg Drokanov) в форменной одежде стоит перед фоном с его логотипом и информацией об аккаунте GitHub. Он смотрит прямо на камеру, улыбается. На заднем плане видны несколько компьютеров и экраны с кодом.\",\n      \"dialogue_text\": \"\",\n      \"image_prompt\": \"A young professional (Oleg Drokanov) stands in front of a background with his GitHub logo and account information. He smiles directly at the camera, wearing formal attire. Multiple computers and screens with code are visible on the background. Photorealistic, cinematic style, shallow depth of field. --ar 16:9 --v 6.0\"\n    },\n    {\n      \"panel_number\": 2,\n      \"visual_description\": \"Молодой парень смотрит на какой-то экран компьютера и улыбается. На заднем плане видны книги по программированию и заметки.\",\n      \"dialogue_text\": \"\",\n      \"image_prompt\": \"A young professional (Oleg Drokanov) looks at a computer screen and smiles. Programming books and notes are visible on the background. A cinematic shot, film noir style, dramatic lighting, shallow depth of field. --ar 16:9 --v 6.0\"\n    },\n    {\n      \"panel_number\": 3,\n      \"visual_description\": \"Портфолио с достижениями и наградами Developer лежит на столе рядом с компьютером.\",\n      \"dialogue_text\": \"\",\n      \"image_prompt\": \"A portfolio with achievements and awards for a developer lies on the desk next to the computer. A cinematic shot, photorealistic style, shallow depth of field. --ar 16:9 --v 6.0\"\n    },\n    {\n      \"panel_number\": 4,\n      \"visual_description\": \"Молодой парень сидит за компьютером и работает с GitHub.\",\n      \"dialogue_text\": \"\",\n      \"image_prompt\": \"A young professional (Oleg Drokanov) sits in front of a computer and works on GitHub. A cinematic shot, film noir style, dramatic lighting, shallow depth of field. --ar 16:9 --v 6.0\"\n    },\n    {\n      \"panel_number\": 5,\n      \"visual_description\": \"Скриншот проекта на GitHub\",\n      \"dialogue_text\": \"\",\n      \"image_prompt\": \"A screenshot of a project on GitHub. A cinematic shot, photorealistic style, shallow depth of field. --ar 16:9 --v 6.0\"\n    }\n  ]\n}\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}